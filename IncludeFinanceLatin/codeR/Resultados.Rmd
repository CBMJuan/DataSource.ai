---
title: "Inclusión Financiera en Latinoamérica"
author: "Edimer (Sidereus)"
output:
  html_notebook:
    code_folding: hide
    df_print: paged
    css: estilo.css
    toc: true
    toc_depth: 5
    toc_float:
      smooth_scroll: false
      collapsed: false
    highlight: breezedark
    
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      fig.align = "center",
                      fig.width = 9,
                      warning = FALSE,
                      message = FALSE)
```

<center>
<img src = "img/img1.png" />
</center>

# Reto

- [Reto en DataSource.ai](https://www.datasource.ai/es/home/competitions/inclusion-financiera-en-latinoamerica)

# Datos

```{r}
library(data.table)
dataTrain <- fread("../data/train.csv")
dataTest <- fread("../data/test.csv")
dataSample <- fread("../data/sample.csv")
```

## Variables Iniciales

- **country:** país. En total hay 4 paises (Argentina, Chile, Colombia, Mexico). El de mayor registros es Chile con el 37.1% y el último es Mexico con 8.9% del total de información.
- **year:** año del registro (2016 a 2018). No hay información de cada país en los tres años, por ejemplo, Argentina sólo tiene información en el año 2018 y Colombia sólo en 2017.
- **uniqueid:** identificador único.
- **bank_account:** acceso a cuenta bancaria (si, no). 85.9% no tienen acceso a cuenta bancaria.
- **location_type:** tipo de ubicación (rural, urbana). El 60.9% de los registros son de ubicación rural.
- **cellphone_acces:** acceso a celular (si, no). El 75% tienen acceso al celular.
- **household_size:** número de personas que viven en una misma casa. Rango de 1 a 21 personas.
- **age_of_respondent:** edad. Rango de 16 a 100 años.
- **gender_of_respondent:** género. 58.9% de los registros son del género femenino.
- **relationship_with_head:** relación con responsable de la familia (cónyuge, hijo, padre, otro pariente, otros no parientes, no sé). El 54.5% de las personas registradas son *cabeza de familia o jefe de hogar*, seguido del 27.7% de *cónyuges* y, en menor cantidad *otros no parientes* con menos del 1%.
- **marital_status:** estado marital (Casado/Viviendo juntos, Divorciado/Separado, Viudo, Soltero/Nunca casado, No sé). El 45.6% de los registros corresponden a personas *casado/viviendo juntos*, seguido del 33.9% de *soltero/nunca casado).
- **education_level:** nivel educativo más alto (Sin educación formal, Educación primaria, Educación secundaria, Formación profesional/Especializada, Educación terciaria, Otro/No sabe). El 54.3% cuentan con educación primaria como nivel máximo de estudio, seguido del 19.1% educados informalmente y 17.9% con educación secundaria. 
- **job_type:** tipo de trabajo (Agricultura y pesca, Autónomo, Empleado formal Gobierno, Empleado formal Privado, Empleado informal, Dependiente de remesas, Dependiente del gobierno, Otros ingresos, Sin ingresos, No sabe/Se niega a responder). El mayor número de registros equivale a *personas que trabajan por cuenta propia* con 27.3%, seguido del *empleo informal* con 23.7%.

## Nuevas variables

- **ageAdult:** si la persona es mayor a 18 años lo categorizo como "si" de lo contrario será "no".
- **sizeFamily:** tamaño del hogar. Creo una nueva variable categórica que clasifica el número de personas que viven en una misma casa. Los niveles son los siguientes:
    - *Solo:* una sóla persona.
    - *Pareja:* dos personas.
    - *F3:* familia de 3 miembros.
    - *F4:* familia de 4 miembros.
    - *F5:* familia mayor o igual a 5 miembros.
- **unemployment:** categorizo una nueva variable para clasificar si la persona está desempleada. Dentro de aquellos desempleados incluyo el trabajo informal independiente, sin ingresos, dependiente de remesas, otros ingresos y los que no responden.
- **usefulLife:** si la persona está desempleada y además está entre 20-60 años de edad, lo clasifico como "Si", de lo contrario será "No".

### Train

```{r}
library(tidyverse)
dataTrain %>% 
  mutate(ageAdult = ifelse(age_of_respondent >= 18, "Si", "No"),
         year = factor(year),
         target = ifelse(bank_account == "Yes", 1, 0),
         sizeFamily = if_else(
           household_size == 1,
           true = "Solo",
           false = if_else(
             household_size == 2,
             true = "Pareja",
             false = if_else(
               household_size == 3,
               true = "F3",
               false = if_else(
                 household_size == 4,
                 true = "F4",
                 false = "F5"
               )
             )
           )
         ),
         unemployment = if_else(
           job_type %in% c("Dont Know/Refuse to answer",
                           "Informally employed",
                           "No Income", "Other Income",
                           "Remittance Dependent"),
           true = "Si",
           false = "No"
         ),
         usefulLife = if_else(
           unemployment == "Si" & age_of_respondent %in% c(20:60),
           true = "Si",
           false = "No"
         )) %>% 
  unite(uniqueid, country, sep = " x ", col = "newID", remove = FALSE) %>% 
  mutate_if(is.character, as.factor) %>% 
  select(-c(bank_account)) %>% 
  select(newID, target, everything()) ->
  myDataTrain
myDataTrain
```

### Test

```{r}
dataTest %>% 
  mutate(ageAdult = ifelse(age_of_respondent >= 18, "Si", "No"),
         year = factor(year),
         sizeFamily = if_else(
           household_size == 1,
           true = "Solo",
           false = if_else(
             household_size == 2,
             true = "Pareja",
             false = if_else(
               household_size == 3,
               true = "F3",
               false = if_else(
                 household_size == 4,
                 true = "F4",
                 false = "F5"
               )
             )
           )
         ),
         unemployment = if_else(
           job_type %in% c("Dont Know/Refuse to answer",
                           "Informally employed",
                           "No Income", "Other Income",
                           "Remittance Dependent"),
           true = "Si",
           false = "No"
         ),
         usefulLife = if_else(
           unemployment == "Si" & age_of_respondent %in% c(20:60),
           true = "Si",
           false = "No"
         )) %>% 
  unite(uniqueid, country, sep = " x ", col = "newID", remove = FALSE) %>% 
  mutate_if(is.character, as.factor) %>% 
  select(newID, everything()) ->
  myDataTest
myDataTest
```

## Exportando datos

```{r}
save(myDataTrain, file = "myDataTrain.Rdata", compress = "xz")
save(myDataTest, file = "myDataTest.Rdata", compress = "xz")
```

# Modelos

- Evalué modelos ajustados con los algoritmos LightGB de Microsoft y CatBoost de Yandex.
- En total envié 10 predicciones. 3 de ellas las obtuve con LightGBM y 6 con CatBoost, con el ensamble de estas 9 prediccciones obtuve la número 10.
- Para algunos modelos permití que el algoritmo por defecto manejará las variables categóricas, sin embargo, para otros codifiqué las variables categóricas previamente, con [*one hot encoding.*](https://en.wikipedia.org/wiki/One-hot)
- LightGBM:
    - [Documentación.](https://lightgbm.readthedocs.io/en/latest/)
    - [Instalación de LightGBM en R.](https://lightgbm.readthedocs.io/en/latest/R/index.html)
    - [Parámetros del algoritmo LightGBM.](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
- CatBoost:
    - [Documentación.](https://catboost.ai/docs/)
    - [Instalación en R.](https://catboost.ai/docs/concepts/r-installation.html)
    - [Parámetros en R.](https://catboost.ai/docs/concepts/r-training-parameters.html)
    
## LightGBM

### Modelo 1

```{r}
# Bibliotecas
library(caret)
library(lightgbm)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid))

# One-Hote Encoding (previous)
library(recipes)
rec <- recipe(target ~ ., data = dataTrain)
dummies <- rec %>% 
  step_dummy(all_nominal(), one_hot = TRUE)
dummies <- prep(dummies, training = dataTrain)
dataTrain2 <- bake(dummies, new_data = dataTrain) %>% 
  select(target, everything()) %>% 
  as.data.frame()

# Datos test
recTest <- recipe(~., data = myDataTest %>% select(-c(newID, uniqueid)))
dumTest <- recTest %>% 
  step_dummy(all_nominal(), one_hot = TRUE)
dumTest <- prep(dumTest, training = myDataTest)
dataTest2 <- bake(dumTest, new_data = myDataTest) %>%  
  as.data.frame()

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain2$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain2[indx, ]
dfTest <- dataTrain2[-indx, ]

# Datos para lightgbm
dataTrain_lgbm <- lgb.Dataset(data = data.matrix(dfTrain[, -1]), 
                              label = dfTrain[, 1])
dataTest_lgbm <- lgb.Dataset(data = data.matrix(dfTest[, -1]),
                             label = dfTest[, 1])

# Parámetros para lightgbm
myParams <- list(
  boosting = "gbdt",
  objective = "binary",
  metric = 'auc',
  learning_rate = 0.01,
  feature_fraction = 1,
  bagging_fraction = 1,
  max_depth = -1,
  is_unbalance = TRUE,
  min_data_in_leaf = 100
)

# Train model
modelo <- lgb.train(params = myParams,
                    data = dataTrain_lgbm,
                    nrounds = 10000,
                    valids = list(test = dataTest_lgbm),
                    early_stopping_rounds = 50)

# best_iteration: 271
# best_score: 0.8660
modelo$best_iter
modelo$best_score

# --------- Predichos test (train)
predicciones0 <- predict(modelo, data.matrix(dfTest %>% 
                                               select(-target)))

# Confusion matrix
cut0.5 <- factor(ifelse(predicciones0 > 0.5, "1", "0"))
confusionMatrix(data = cut0.5,
                reference = factor(dfTest$target),
                positive = "1")

# --------- Predichos test (submission)

# Predicciones
predicciones1 <- predict(modelo, data.matrix(dataTest2))


# Submission
dataLGBM1 <- myDataTest %>% 
  mutate(bank_account = if_else(predicciones1 > 0.5, "1", "0")) %>% 
  select(uniqueid = newID, bank_account)
write.csv(dataLGBM1, file = "submission/lgbm1.csv", row.names = FALSE)

```

### Modelo 2

```{r}
# Bibliotecas
library(caret)
library(lightgbm)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid))

# One-Hote Encoding (previous)
library(recipes)
rec <- recipe(target ~ ., data = dataTrain)
dummies <- rec %>% 
  step_dummy(all_nominal(), one_hot = TRUE)
dummies <- prep(dummies, training = dataTrain)
dataTrain2 <- bake(dummies, new_data = dataTrain) %>% 
  select(target, everything()) %>% 
  as.data.frame()

# Datos test
recTest <- recipe(~., data = myDataTest %>% select(-c(newID, uniqueid)))
dumTest <- recTest %>% 
  step_dummy(all_nominal(), one_hot = TRUE)
dumTest <- prep(dumTest, training = myDataTest)
dataTest2 <- bake(dumTest, new_data = myDataTest) %>%  
  as.data.frame()

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain2$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain2[indx, ]
dfTest <- dataTrain2[-indx, ]

# Datos para lightgbm
dataTrain_lgbm <- lgb.Dataset(data = data.matrix(dfTrain[, -1]), 
                              label = dfTrain[, 1])
dataTest_lgbm <- lgb.Dataset(data = data.matrix(dfTest[, -1]),
                             label = dfTest[, 1])

# Parámetros para lightgbm
myParams <- list(
  boosting = "gbdt",
  objective = "binary",
  metric = 'auc',
  learning_rate = 0.001,
  feature_fraction = 1,
  bagging_fraction = 1,
  max_depth = -1,
  is_unbalance = TRUE,
  min_data_in_leaf = 100
)

# Train model
modelo <- lgb.train(params = myParams,
                    data = dataTrain_lgbm,
                    nrounds = 10000,
                    valids = list(test = dataTest_lgbm),
                    early_stopping_rounds = 50)

# best_iteration: 1757
# best_score: 0.8647503
modelo$best_iter
modelo$best_score

# --------- Predichos test (train)
predicciones0 <- predict(modelo, data.matrix(dfTest %>% 
                                               select(-target)))

# Confusion matrix
cut0.5 <- factor(ifelse(predicciones0 > 0.5, "1", "0"))
confusionMatrix(data = cut0.5,
                reference = factor(dfTest$target),
                positive = "1")

# --------- Predichos test (submission)

# Predicciones
predicciones1 <- predict(modelo, data.matrix(dataTest2))


# Submission
dataLGBM3 <- myDataTest %>% 
  mutate(bank_account = if_else(predicciones1 > 0.5, "1", "0")) %>% 
  select(uniqueid = newID, bank_account)
write.csv(dataLGBM3, file = "submission/lgbm3.csv", row.names = FALSE)
```

### Modelo 3

```{r}
# Cargando datos
library(data.table)
library(tidyverse)
load("myDataTrain.Rdata")
load("myDataTest.Rdata")
dataSample <- fread("../data/sample.csv")


# Bibliotecas
library(caret)
library(lightgbm)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid))

# One-Hote Encoding (previous)
library(recipes)
rec <- recipe(target ~ ., data = dataTrain)
dummies <- rec %>% 
  step_dummy(all_nominal(), one_hot = TRUE)
dummies <- prep(dummies, training = dataTrain)
dataTrain2 <- bake(dummies, new_data = dataTrain) %>% 
  select(target, everything()) %>% 
  as.data.frame()

# Datos test
recTest <- recipe(~., data = myDataTest %>% select(-c(newID, uniqueid)))
dumTest <- recTest %>% 
  step_dummy(all_nominal(), one_hot = TRUE)
dumTest <- prep(dumTest, training = myDataTest)
dataTest2 <- bake(dumTest, new_data = myDataTest) %>%  
  as.data.frame()

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain2$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain2[indx, ]
dfTest <- dataTrain2[-indx, ]

# Datos para lightgbm
dataTrain_lgbm <- lgb.Dataset(data = data.matrix(dfTrain[, -1]), 
                              label = dfTrain[, 1])
dataTest_lgbm <- lgb.Dataset(data = data.matrix(dfTest[, -1]),
                             label = dfTest[, 1])

# Parámetros para lightgbm
myParams <- list(
  boosting = "gbdt",
  objective = "binary",
  metric = 'auc',
  learning_rate = 0.001,
  feature_fraction = 1,
  bagging_fraction = 1,
  max_depth = -1,
  is_unbalance = TRUE,
  min_data_in_leaf = 100,
  num_leaves = 64,
  subsample_for_bin = 200,
  reg_alpha = 1.2,
  reg_lambda = 1.2,
  min_split_gain = 0.5,
  min_child_weight = 1,
  min_child_samples = 5,
  scale_pos_weight = 1,
  num_class = 1
)

# Train model
modelo <- lgb.train(params = myParams,
                    data = dataTrain_lgbm,
                    nrounds = 10000,
                    valids = list(test = dataTest_lgbm),
                    early_stopping_rounds = 50)

# best_iteration: 935
# best_score: 0.8558094
modelo$best_iter
modelo$best_score

# --------- Predichos test (train)
predicciones0 <- predict(modelo, data.matrix(dfTest %>% 
                                               select(-target)))

# Confusion matrix
cut0.5 <- factor(ifelse(predicciones0 > 0.5, "1", "0"))
confusionMatrix(data = cut0.5,
                reference = factor(dfTest$target),
                positive = "1")

# --------- Predichos test (submission)

# Predicciones
predicciones1 <- predict(modelo, data.matrix(dataTest2))


# Submission
dataLGBM4 <- myDataTest %>% 
  mutate(bank_account = if_else(predicciones1 > 0.5, "1", "0")) %>% 
  select(uniqueid = newID, bank_account)
write.csv(dataLGBM4, file = "submission/lgbm4.csv", row.names = FALSE)

```

## CatBoost

### Modelo 1

```{r}
# Bibliotecas
library(caret)
library(catboost)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid))

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain[indx, ]
dfTest <- dataTrain[-indx, ]

# Datos para catboost
train_pool <- catboost.load_pool(data = dfTrain[, -1], label = dfTrain[, 1])
test_pool <- catboost.load_pool(data = dfTest[, -1], label = dfTest[, 1])

# Ajuste de modelo
fit_params <- list(iterations = 100,
                   thread_count = 10,
                   loss_function = 'Logloss',
                   ignored_features = c(4,9),
                   border_count = 32,
                   depth = 5,
                   learning_rate = 0.03,
                   l2_leaf_reg = 3.5,
                   train_dir = 'train_dir',
                   logging_level = 'Silent')
modelo <- catboost.train(train_pool, test_pool, fit_params)

# Predicciones
predicciones1 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Probability')

predicciones2 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Class')

# Confusion matrix
confusionMatrix(data = factor(predicciones2),
                reference = factor(dfTest$target),
                positive = "1")

# Predicciones submission
myDataTest2 <- myDataTest %>% select(-c(newID, uniqueid))
test_pool2 <- catboost.load_pool(myDataTest2)  
predicciones3 <- catboost.predict(modelo, test_pool2,
                                  prediction_type = 'Class')

# Submission
dataCatboost1 <- myDataTest %>% 
  mutate(bank_account = predicciones3) %>% 
  select(uniqueid = newID, bank_account)
write.csv(dataCatboost1 , file = "submission/catB1.csv", row.names = FALSE)

```

### Modelo 2

```{r}
# Bibliotecas
library(caret)
library(catboost)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid))

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain[indx, ]
dfTest <- dataTrain[-indx, ]

# Datos para catboost
train_pool <- catboost.load_pool(data = dfTrain[, -1], label = dfTrain[, 1])
test_pool <- catboost.load_pool(data = dfTest[, -1], label = dfTest[, 1])

# Ajuste de modelo
fit_params <- list(iterations = 1000,
                   loss_function = 'Logloss',
                   depth = 8,
                   border_count = 64,
                   l2_leaf_reg = 3,
                   learning_rate = 0.01)
modelo <- catboost.train(train_pool, test_pool, fit_params)

# Predicciones
predicciones1 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Probability')

predicciones2 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Class')

# Confusion matrix
confusionMatrix(data = factor(predicciones2),
                reference = factor(dfTest$target),
                positive = "1")

# Accuracy: 0.8875
# Kappa: 0.4238

# Predicciones submission
myDataTest2 <- myDataTest %>% select(-c(newID, uniqueid))
test_pool2 <- catboost.load_pool(myDataTest2)  
predicciones3 <- catboost.predict(modelo, test_pool2,
                                  prediction_type = 'Class')

# Submission
dataCatboost2 <- myDataTest %>% 
  mutate(bank_account = predicciones3) %>% 
  select(uniqueid = newID, bank_account)
write.csv(dataCatboost2, file = "submission/catB2.csv", row.names = FALSE)
```

### Modelo 3

```{r}
# Bibliotecas
library(caret)
library(catboost)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid))

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain[indx, ]
dfTest <- dataTrain[-indx, ]

# Datos para catboost
train_pool <- catboost.load_pool(data = dfTrain[, -1], label = dfTrain[, 1])
test_pool <- catboost.load_pool(data = dfTest[, -1], label = dfTest[, 1])

# Ajuste de modelo
fit_params <- list(iterations = 1000,
                   loss_function = 'Logloss',
                   depth = 10,
                   border_count = 128,
                   l2_leaf_reg = 5,
                   learning_rate = 0.01)
modelo <- catboost.train(train_pool, test_pool, fit_params)

# Predicciones
predicciones1 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Probability')

predicciones2 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Class')

# Confusion matrix
confusionMatrix(data = factor(predicciones2),
                reference = factor(dfTest$target),
                positive = "1")

# Accuracy: 0.8873
# Kappa: 0.4158

# Predicciones submission
myDataTest2 <- myDataTest %>% select(-c(newID, uniqueid))
test_pool2 <- catboost.load_pool(myDataTest2)  
predicciones3 <- catboost.predict(modelo, test_pool2,
                                  prediction_type = 'Class')

# Submission
dataCatboost3 <- myDataTest %>% 
  mutate(bank_account = predicciones3) %>% 
  select(uniqueid = newID, bank_account)
write.csv(dataCatboost3, file = "submission/catB3.csv", row.names = FALSE)

```

### Modelo 4

```{r}
# Bibliotecas
library(caret)
library(catboost)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid)) %>% 
  mutate(target = ifelse(target == 1, "Si", "No"))

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain[indx, ]
dfTest <- dataTrain[-indx, ]

# Ajuste de modelo
fit_control <- trainControl(
  method = "cv", 
  number = 5,
  search = "random",
  classProbs = TRUE
)
# set grid options
grid <- expand.grid(
  depth = c(6, 8, 15),
  learning_rate = 0.1,
  l2_leaf_reg = c(3, 5),
  rsm = c(1, 0.5, 0.3),
  border_count = c(32, 128),
  iterations = 100
)
model <- caret::train(
  x = dfTrain[, -1], 
  y = dfTrain[, 1],
  method = catboost.caret,
  metric = "Accuracy",
  maximize = TRUE,
  tuneGrid = grid, 
  trControl = fit_control
)

# Best tuning
model$bestTune

# Predicciones
predicciones1 <- predict(model, newdata = dfTest, type = "prob")
predicciones2 <- ifelse(predicciones1$Si > 0.5, "Si", "No")

# Confusion matrix
confusionMatrix(data = factor(predicciones2),
                reference = factor(dfTest$target),
                positive = "Si")

# Accuracy: 0.8835
# Kappa: 0.3915

# Predicciones submission
predicciones3 <- predict(model, newdata = myDataTest, type = "prob")
predicciones4 <- ifelse(predicciones3$Si > 0.5, "Si", "No")

# Submission
dataCatboost4 <- myDataTest %>% 
  mutate(bank_account = predicciones4) %>% 
  select(uniqueid = newID, bank_account) %>% 
  mutate(bank_account = ifelse(bank_account == "Si", "1", "0"))
write.csv(dataCatboost4, file = "submission/catB4.csv", row.names = FALSE)
```


### Modelo 5

```{r}
# Bibliotecas
library(caret)
library(catboost)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid))

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain[indx, ]
dfTest <- dataTrain[-indx, ]

# Datos para catboost
train_pool <- catboost.load_pool(data = dfTrain[, -1], label = dfTrain[, 1])
test_pool <- catboost.load_pool(data = dfTest[, -1], label = dfTest[, 1])

# Ajuste de modelo
fit_params <- list(iterations = 5000,
                   loss_function = 'Logloss',
                   depth = 6,
                   border_count = 32,
                   rsm = 0.5,
                   l2_leaf_reg = 3,
                   learning_rate = 0.01,
                   od_type = 'Iter',
                   use_best_model = TRUE,
                   od_wait = 500)
modelo <- catboost.train(train_pool, test_pool, fit_params)

# Predicciones
predicciones1 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Probability')

predicciones2 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Class')

# Confusion matrix
confusionMatrix(data = factor(predicciones2),
                reference = factor(dfTest$target),
                positive = "1")

# Accuracy: 0.8869
# Kappa: 0.4209

# Predicciones submission
myDataTest2 <- myDataTest %>% select(-c(newID, uniqueid))
test_pool2 <- catboost.load_pool(myDataTest2)  
predicciones3 <- catboost.predict(modelo, test_pool2,
                                  prediction_type = 'Class')

# Submission
dataCatboost5 <- myDataTest %>% 
  mutate(bank_account = predicciones3) %>% 
  select(uniqueid = newID, bank_account)
write.csv(dataCatboost5, file = "submission/catB5.csv", row.names = FALSE)
```

### Modelo 6

```{r}
# Bibliotecas
library(caret)
library(catboost)

# Datos train
dataTrain <- as.data.frame(myDataTrain) %>% 
  select(-c(newID, uniqueid))

# One-Hote Encoding (previous)
library(recipes)
rec <- recipe(target ~ ., data = dataTrain)
dummies <- rec %>% 
  step_dummy(all_nominal(), one_hot = TRUE)
dummies <- prep(dummies, training = dataTrain)
dataTrain2 <- bake(dummies, new_data = dataTrain) %>% 
  select(target, everything()) %>% 
  as.data.frame()

# Datos test
recTest <- recipe(~., data = myDataTest %>% select(-c(newID, uniqueid)))
dumTest <- recTest %>% 
  step_dummy(all_nominal(), one_hot = TRUE)
dumTest <- prep(dumTest, training = myDataTest)
dataTest2 <- bake(dumTest, new_data = myDataTest) %>%  
  as.data.frame()

# Partición de datos (80% train - 20% test)
set.seed(123)
indx <- createDataPartition(y = dataTrain2$target, times = 1, p = 0.8, list = FALSE)
dfTrain <- dataTrain2[indx, ]
dfTest <- dataTrain2[-indx, ]

# Datos para catboost
train_pool <- catboost.load_pool(data = dfTrain[, -1], label = dfTrain[, 1])
test_pool <- catboost.load_pool(data = dfTest[, -1], label = dfTest[, 1])

# Ajuste de modelo
fit_params <- list(iterations = 5000,
                   loss_function = 'Logloss',
                   depth = 6,
                   border_count = 32,
                   rsm = 0.5,
                   l2_leaf_reg = 3,
                   learning_rate = 0.01,
                   od_type = 'Iter',
                   use_best_model = TRUE,
                   od_wait = 500)
modelo <- catboost.train(train_pool, test_pool, fit_params)

# Predicciones
predicciones1 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Probability')

predicciones2 <- catboost.predict(modelo, test_pool,
                                  prediction_type = 'Class')

# Confusion matrix
confusionMatrix(data = factor(predicciones2),
                reference = factor(dfTest$target),
                positive = "1")

# Accuracy: 0.8873
# Kappa: 0.4186

# Predicciones submission
test_pool2 <- catboost.load_pool(dataTest2)  
predicciones3 <- catboost.predict(modelo, test_pool2,
                                  prediction_type = 'Class')

# Submission
dataCatboost6 <- myDataTest %>% 
  mutate(bank_account = predicciones3) %>% 
  select(uniqueid = newID, bank_account)
write.csv(dataCatboost6, file = "submission/catB6.csv", row.names = FALSE)
```

## Ensamble

```{r}
# All predictions
lgbm1 <- read.csv("submission/lgbm1.csv")
lgbm2 <- read.csv("submission/lgbm3.csv")
lgbm3 <- read.csv("submission/lgbm4.csv")
catb1 <- read.csv("submission/catB1.csv")
catb2 <- read.csv("submission/catB2.csv")
catb3 <- read.csv("submission/catB3.csv")
catb4 <- read.csv("submission/catB4.csv")
catb5 <- read.csv("submission/catB5.csv")
catb6 <- read.csv("submission/catB6.csv")

# Join data
oneData <- data.frame(
  uniqueid = lgbm1$uniqueid,
  lgbm1 = lgbm1$bank_account,
  lgbm2 = lgbm2$bank_account,
  lgbm3 = lgbm3$bank_account,
  catb1 = catb1$bank_account,
  catb2 = catb2$bank_account,
  catb3 = catb3$bank_account,
  catb4 = catb4$bank_account,
  catb5 = catb5$bank_account,
  catb6 = catb6$bank_account
)

# One prediction
dataEnsemble <- data.frame(
  uniqueid = oneData$uniqueid,
  bank_account = ifelse(apply(oneData[, -1], 1, sum) >= 5, "1", "0")
)

write.csv(dataEnsemble, file = "submission/ensemble.csv", row.names = FALSE)
```

